# -*- coding: utf-8 -*-
"""By Category Language Analysis FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pllJ0Jwt6aqOAwsoqO2U4lY-ecsMuQ_m

*   Break it down by category - words commonly used for each category
*   Language analysis on chatbot

ðŸ’¡ Request Intent Classification
Cluster or categorize comments by intent manually or via a classifier:
- Request for help
- Information seeking
- Compliment
- Complaint

ðŸ“… Temporal Language Trends
- Track how certain words or topics rise/fall over time (Oct â†’ Feb).
- Useful if your contact request trends vary by season or campaign.

## Set-Up
"""

!pip install wordcloud matplotlib
!pip install hdbscan
!pip install scikit-learn
!pip install keybert

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import hdbscan
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict

# Import contact requests

requests = pd.read_excel('/content/updated_senior_source_data.xlsx')

# Rename column headers
requests = requests.rename(columns={'AREA WITH WHICH YOU NEED ASSISTANCE': 'ASSISTANCE TYPE',
                                    'Discovery Method': 'DISCOVERY METHOD'})

# Remove column
requests = requests.drop('HOW DID YOU HEAR ABOUT THE SENIOR SOURCE?', axis=1)

# Convert 'ENTRY DATE' to datetime
requests['ENTRY DATE'] = pd.to_datetime(requests['ENTRY DATE'])

# Create TIME column from ENTRY DATE
requests['TIME'] = requests['ENTRY DATE'].dt.time

requests.head()

date = requests['ENTRY DATE'].dt.date
date_counts = date.value_counts().sort_index().reset_index()
print(date_counts)

age = requests['AGE RANGE']
age_counts = age.value_counts().sort_index().reset_index()
print(age_counts)

category = requests['ASSISTANCE TYPE']
category_counts = category.value_counts().sort_index().reset_index()
print(category_counts)

"""# Language Analysis

Basic NLP Models:

* TF-IDF (Term Frequency-Inverse Document Frequency): Good for identifying
frequently used terms while filtering out common words that arenâ€™t meaningful.
* n-gram Analysis: Useful for finding common word sequences (e.g., "need help with," "how to apply").

Pretrained Language Models:

* BERT (Bidirectional Encoder Representations from Transformers): Can be fine-tuned to analyze and categorize user intent and detect commonly used phrases.
* GPT (such as GPT-4 or GPT-3.5): Can be used to summarize key themes and extract recurring phrases from chatbot conversations.

Topic Modeling Approaches:

* Latent Dirichlet Allocation (LDA): A probabilistic model that can identify
common topics from chatbot messages.

* BERTopic: A modern topic modeling technique that leverages transformer models to find common themes in text.

Text Clustering & Keyword Extraction:

* KeyBERT: A BERT-based approach to extract the most relevant keywords from chatbot interactions.
* SpaCy & Named Entity Recognition (NER): Can help identify key terms related to services seniors ask about.

## Using TF-IDF and BERTopic

### TF-IDF and BERTopic Set-Up
"""

!pip install bertopic
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from bertopic import BERTopic
from keybert import KeyBERT

# Download stopwords (common, irrelevant words) from NLTK
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

print(requests['ASSISTANCE TYPE'].unique())

placeholder_words = ['name','address','phone','email','website','location','company']

def preprocess_text(text):
    text = text.lower()
    # Remove numbers
    # text = re.sub(r'\d+', '', text)
    text = re.sub(r'[0-9]+', '', text)
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Remove stopwords and placeholder words
    text = " ".join([word for word in text.split() if word not in stop_words and word not in placeholder_words])
    return text

"""### Using TF-IDF and BERTopic by Assistance Type"""

!apt install fonts-dejavu
font_path='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf'

!pip install matplotlib
import matplotlib
from matplotlib import colormaps  # To access and modify colormaps
import textwrap

# Get the viridis colormap
viridis = colormaps['viridis']

# Create a custom colormap by truncating viridis to exclude lighter greens and yellows
# Use only the first 60% of the colormap (0.0 to 0.6) to avoid yellows and light greens
colors = viridis(np.linspace(0, 0.9, 256))  # Truncate the colormap
custom_viridis = matplotlib.colors.LinearSegmentedColormap.from_list("custom_viridis", colors)

common_words_by_assistance_type = {}

for assitance_type in requests['ASSISTANCE TYPE'].unique():
    assitance_type_requests = requests[requests['ASSISTANCE TYPE'] == assitance_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    print(assitance_type, '\n')

    # Prepare comments for the model
    comments = assitance_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    # Use TF-IDF model to identify the frequently used words
    vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=50)
    X = vectorizer.fit_transform(preprocessed_comments)
    common_words = vectorizer.get_feature_names_out()

    print("Common Words/Phrases:", common_words)

    common_words_by_assistance_type[assitance_type] = list(common_words)

    # Use BERTopic to find different groups or topics in the comments
    # Adjust min_cluster_size to be less than or equal to the number of preprocessed comments
    topic_model = BERTopic(hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min(5, len(preprocessed_comments)),min_samples=min(5, len(preprocessed_comments)))) #added code: changed default min cluster size
    topics, _ = topic_model.fit_transform(preprocessed_comments)
    df_topics = pd.DataFrame({'Topic': topics, 'Comment': preprocessed_comments})
    topic_model.get_topic_info()

    # Combine all cleaned messages into a single text
    all_text = ' '.join(preprocessed_comments)

    # Manually remove words
    custom_stopwords = {}

    # Filter out the unwanted words
    filtered_text = " ".join([word for word in all_text.split() if word not in custom_stopwords])

    # Create the word cloud
    wordcloud = WordCloud(width=800, height=700,
                          background_color="whitesmoke",
                          colormap=custom_viridis,
                          min_font_size=16,
                          font_path=font_path,
                          max_words=100).generate(filtered_text)

    # Display the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
  #  wrapped_title = textwrap.fill(assitance_type.upper(), width=40)
  #  plt.title(f"â€” {wrapped_title} â€”", fontweight='bold')
    plt.show()

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for assistance_type1 in common_words_by_assistance_type:
    for assistance_type2 in common_words_by_assistance_type:
        if assistance_type1 != assistance_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(common_words_by_assistance_type[assistance_type1]) & set(common_words_by_assistance_type[assistance_type2])

            # Store the number of common words
            common_word_counts[assistance_type1][assistance_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of Common Words between Assistance Types")
plt.show()

from collections import Counter

for assitance_type in requests['ASSISTANCE TYPE'].unique():
    assitance_type_requests = requests[requests['ASSISTANCE TYPE'] == assitance_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    print(assitance_type, '\n')

    # Prepare comments for the model
    comments = assitance_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    # Create BERTopic model
    # Use BERTopic to find different groups or topics in the comments
    # Adjust min_cluster_size and min_samples to be less than or equal to the number of preprocessed comments
    topic_model = BERTopic(hdbscan_model=hdbscan.HDBSCAN(
        min_cluster_size=min(5, len(preprocessed_comments)),
        min_samples=min(2, len(preprocessed_comments))  # Changed min_samples here
    ))
    topics, _ = topic_model.fit_transform(preprocessed_comments)

    # Get top topics
    topic_info = topic_model.get_topic_info()
    print(topic_info.head(5))

    kw_model = KeyBERT()

    # Convert the Series to a DataFrame with a 'text' column
    keybert_comments = preprocessed_comments.to_frame(name='text')


    # Option 1: Extract global keywords from the entire dataset
    full_text = " ".join(keybert_comments['text']) # Changed to join text column
    global_keywords = kw_model.extract_keywords(
        full_text,
        keyphrase_ngram_range=(1, 2),
        stop_words="english",
        top_n=10,
        diversity=0.7
    )

    # Display Global Keywords
    print("\n===== Global Keywords (from entire dataset) =====")
    for keyword, score in global_keywords:
        print(f"{keyword}: {score:.4f}")

    # Option 2: Extract keywords per entry and aggregate
    all_keywords = []
    for doc in keybert_comments['text']: # Changed documents to keybert_comments['text']
        keywords = kw_model.extract_keywords(
            doc,
            keyphrase_ngram_range=(1, 2),
            stop_words="english",
            top_n=5
        )
        all_keywords.extend([kw[0] for kw in keywords])

    top_keywords = Counter(all_keywords).most_common(10)

    # Display Aggregated Keywords
    print("\n===== Aggregated Keywords (from individual entries) =====")
    for keyword, count in top_keywords:
        print(f"{keyword}: {count}")

    # Extract top 5 keywords per comment
    # Join the list of keywords into a string
    keybert_comments['keywords'] = keybert_comments['text'].apply(
        lambda x: ', '.join([keyword for keyword, score in kw_model.extract_keywords(
            x, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5
        )])
    )

    # Display the first few rows
    print(keybert_comments[['text', 'keywords']].head())

"""### Using TF-IDF and BERTopic by Age"""

common_words_by_age_type = {}

for age_type in requests['AGE RANGE'].unique():
    age_type_requests = requests[requests['AGE RANGE'] == age_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    print(age_type, '\n\n')

    # Prepare comments for the model
    comments = age_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    # Use TF-IDF model to identify the frequently used words
    vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=50)
    X = vectorizer.fit_transform(preprocessed_comments)
    common_words = vectorizer.get_feature_names_out()

    print("Common Words/Phrases:", common_words)

    common_words_by_age_type[age_type] = list(common_words)

    # Use BERTopic to find different groups or topics in the comments
    # Adjust min_cluster_size to be less than or equal to the number of preprocessed comments
    topic_model = BERTopic(hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min(5, len(preprocessed_comments)),min_samples=min(5, len(preprocessed_comments)))) #added code: changed default min cluster size
    topics, _ = topic_model.fit_transform(preprocessed_comments)
    df_topics = pd.DataFrame({'Topic': topics, 'Comment': preprocessed_comments})
    topic_model.get_topic_info()

    # Combine all cleaned messages into a single text
    all_text = ' '.join(preprocessed_comments)

    # Manually remove words
    custom_stopwords = {}

    # Filter out the unwanted words
    filtered_text = " ".join([word for word in all_text.split() if word not in custom_stopwords])

    # Create the word cloud
    #wordcloud = WordCloud(width=800, height=400, background_color="white", colormap="viridis", max_words=100).generate(filtered_text)
    wordcloud = WordCloud(width=800, height=700,
                          background_color="whitesmoke",
                          colormap=custom_viridis,
                          min_font_size=16,
                          font_path=font_path,
                          max_words=100).generate(filtered_text)

    # Display the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
   #plt.title(f"Most Common Words for {age_type}")
    plt.show()

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for age_type1 in common_words_by_age_type:
    for age_type2 in common_words_by_age_type:
        if age_type1 != age_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(common_words_by_age_type[age_type1]) & set(common_words_by_age_type[age_type2])

            # Store the number of common words
            common_word_counts[age_type1][age_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of Common Words between Age Types")
plt.show()

"""### Using TF-IDF and BERTopic by Priority Score"""

common_words_by_priority_type = {}

for priority_type in requests['Priority Score'].unique():
    priority_type_requests = requests[requests['Priority Score'] == priority_type]
    # print(priority_type, '\n', priority_type_requests['COMMENTS'].head())

    print(priority_type, '\n\n')

    # Prepare comments for the model
    comments = priority_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    # Use TF-IDF model to identify the frequently used words
    vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=50)
    X = vectorizer.fit_transform(preprocessed_comments)
    common_words = vectorizer.get_feature_names_out()

    print("Common Words/Phrases:", common_words)

    common_words_by_priority_type[priority_type] = list(common_words)

    # Use BERTopic to find different groups or topics in the comments
    # Adjust min_cluster_size to be less than or equal to the number of preprocessed comments
    topic_model = BERTopic(hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min(5, len(preprocessed_comments)),min_samples=min(5, len(preprocessed_comments)))) #added code: changed default min cluster size
    topics, _ = topic_model.fit_transform(preprocessed_comments)
    df_topics = pd.DataFrame({'Topic': topics, 'Comment': preprocessed_comments})
    topic_model.get_topic_info()

    # Combine all cleaned messages into a single text
    all_text = ' '.join(preprocessed_comments)

    # Manually remove words
    custom_stopwords = {}

    # Filter out the unwanted words
    filtered_text = " ".join([word for word in all_text.split() if word not in custom_stopwords])

    # Create the word cloud
    #wordcloud = WordCloud(width=800, height=400, background_color="white", colormap="viridis", max_words=100).generate(filtered_text)

    # Create the word cloud
    wordcloud = WordCloud(width=800, height=700,
                          background_color="whitesmoke",
                          colormap=custom_viridis,
                          min_font_size=16,
                          font_path=font_path,
                          max_words=100).generate(filtered_text)

    # Display the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
  #  plt.title(f"Most Common Words for Priority Score {priority_type}")
    plt.show()

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for age_type1 in common_words_by_age_type:
    for age_type2 in common_words_by_age_type:
        if age_type1 != age_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(common_words_by_age_type[age_type1]) & set(common_words_by_age_type[age_type2])

            # Store the number of common words
            common_word_counts[age_type1][age_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of Common Words between Priority Score")
plt.show()

"""### Using TF-IDF and BERTopic by Month"""

common_words_by_month_type = {}

month_year = requests
month_year['MONTH & YEAR'] = month_year['MONTH'] + ' ' + month_year['YEAR'].astype(str)

for month_type in month_year['MONTH & YEAR'].unique():
    month_type_requests = month_year[month_year['MONTH & YEAR'] == month_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    print(month_type, '\n\n')

    # Prepare comments for the model
    comments = month_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    # Use TF-IDF model to identify the frequently used words
    vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=50)
    X = vectorizer.fit_transform(preprocessed_comments)
    common_words = vectorizer.get_feature_names_out()

    print("Common Words/Phrases:", common_words)

    common_words_by_month_type[month_type] = list(common_words)

    # Use BERTopic to find different groups or topics in the comments
    # Adjust min_cluster_size to be less than or equal to the number of preprocessed comments
    topic_model = BERTopic(hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min(5, len(preprocessed_comments)),min_samples=min(5, len(preprocessed_comments)))) #added code: changed default min cluster size
    topics, _ = topic_model.fit_transform(preprocessed_comments)
    df_topics = pd.DataFrame({'Topic': topics, 'Comment': preprocessed_comments})
    topic_model.get_topic_info()

    # Combine all cleaned messages into a single text
    all_text = ' '.join(preprocessed_comments)

    # Manually remove words
    custom_stopwords = {}

    # Filter out the unwanted words
    filtered_text = " ".join([word for word in all_text.split() if word not in custom_stopwords])

    # Create the word cloud
    #wordcloud = WordCloud(width=800, height=400, background_color="white", colormap="viridis", max_words=100).generate(filtered_text)

    # Create the word cloud
    wordcloud = WordCloud(width=800, height=700,
                          background_color="whitesmoke",
                          colormap=custom_viridis,
                          min_font_size=16,
                          font_path=font_path,
                          max_words=100).generate(filtered_text)

    # Display the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
  #  plt.title(f"Most Common Words for {month_type}")
    plt.show()

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for assistance_type1 in common_words_by_month_type:
    for assistance_type2 in common_words_by_month_type:
        if assistance_type1 != assistance_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(common_words_by_month_type[assistance_type1]) & set(common_words_by_month_type[assistance_type2])

            # Store the number of common words
            common_word_counts[assistance_type1][assistance_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df.sort_values(by=common_word_counts_df.columns[0],ascending=False)
, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display #Changed 'sort_value()' to 'sort_values()' and specified the by argument
plt.title("Number of Common Words between Month Types")
plt.show()

"""## N-gram Analysis - With Stopwords Included

### N-gram Analysis - With Stopwords Included Set-Up
"""

## STILL NEEDS TO BE FIXED - SUB-GRAMS ARE NOT BEING REMOVED

def remove_redundant_ngrams(ngrams):
    filtered_ngrams = []
    for ngram, freq in ngrams:
        ngram_tokens = set(ngram.split())

        # Check if n-gram is fully contained in a longer n-gram
        if not any(ngram_tokens.issubset(set(longer_ngram.split())) for longer_ngram, _ in filtered_ngrams):
            filtered_ngrams.append((ngram, freq))

    return filtered_ngrams

from collections import Counter
from nltk import ngrams
import matplotlib.pyplot as plt

"""### N-gram Analysis - With Stopwords Included by Assistance Type"""

ngram_df_by_assistance_type = {}

for assitance_type in requests['ASSISTANCE TYPE'].unique():
    assitance_type_requests = requests[requests['ASSISTANCE TYPE'] == assitance_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    # Prepare comments for the model
    comments = assitance_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=30)
    X = vectorizer.fit_transform(comments)

    # Get n-gram frequency
    ngram_counts = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))

    # Sort n-grams by frequency
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    # Apply filtering
    filtered_ngrams = remove_redundant_ngrams(sorted_ngrams)

    # Convert to DataFrame for visualization
    ngram_df = pd.DataFrame(filtered_ngrams, columns=["N-gram", "Frequency"])

    ngram_df_by_assistance_type[assitance_type] = list(ngram_df['N-gram'])

    print('\n\n', assitance_type, '\n\n')

    # Plot the most common n-grams
    # Dynamic figure height based on number of bars
    bar_height = 0.5  # Reduce this to add more space between bars
    num_bars = len(ngram_df)
    fig_height = num_bars * bar_height *0.36  # 0.2 adds spacing
    plt.figure(figsize=(10, fig_height))
    #plt.figure(figsize=(10, 7))
    #plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")

    bars = plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue", height=bar_height)
    plt.bar_label(bars, fmt='%d', padding=3, color='darkblue', fontsize=8)

    plt.xlabel("Frequency")
    plt.ylabel("Common Phrases")
   # plt.title(f"Frequent Phrases (Including Stopwords) in Contact Forms for {assitance_type}")

    plt.gca().set_ylim(-0.5, num_bars - 0.5)  # Trim top and bottom whitespace

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.show()

    # Print the top n-grams
    print("Top N-grams:\n", ngram_df)

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for assistance_type1 in ngram_df_by_assistance_type:
    for assistance_type2 in ngram_df_by_assistance_type:
        if assistance_type1 != assistance_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(ngram_df_by_assistance_type[assistance_type1]) & set(ngram_df_by_assistance_type[assistance_type2])

            # Store the number of common words
            common_word_counts[assistance_type1][assistance_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of N-Grams between Assistance Types")
plt.show()

"""### N-gram Analysis - With Stopwords Included by Age Type"""

ngram_df_by_age_type = {}

for age_type in requests['AGE RANGE'].unique():
    age_type_requests = requests[requests['AGE RANGE'] == age_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    # Prepare comments for the model
    comments = age_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=30)
    X = vectorizer.fit_transform(comments)

    # Get n-gram frequency
    ngram_counts = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))

    # Sort n-grams by frequency
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    # Apply filtering
    filtered_ngrams = remove_redundant_ngrams(sorted_ngrams)

    # Convert to DataFrame for visualization
    ngram_df = pd.DataFrame(filtered_ngrams, columns=["N-gram", "Frequency"])

    ngram_df_by_age_type[age_type] = list(ngram_df['N-gram'])

    # # Plot the most common n-grams
    # plt.figure(figsize=(10, 5))
    # plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")
    # plt.xlabel("Frequency")
    # plt.ylabel("Common Phrases")
    # plt.title(f"Frequent Phrases (Including Stopwords) in Contact Forms for {age_type}")
    # plt.gca().invert_yaxis()
    # plt.show()

    # # Print the top n-grams
    # print("Top N-grams:\n", ngram_df)

    print('\n\n', age_type, '\n\n')


    # Dynamic figure height based on number of bars
    bar_height = 0.5  # Reduce this to add more space between bars
    num_bars = len(ngram_df)
    fig_height = num_bars * bar_height *0.36  # 0.2 adds spacing
    plt.figure(figsize=(10, fig_height))
    #plt.figure(figsize=(10, 7))
    #plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")

    bars = plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue", height=bar_height)
    plt.bar_label(bars, fmt='%d', padding=3, color='darkblue', fontsize=8)

    plt.xlabel("Frequency")
    plt.ylabel("Common Phrases")
   # plt.title(f"Frequent Phrases (Including Stopwords) in Contact Forms for {age_type}")

    plt.gca().set_ylim(-0.5, num_bars - 0.5)  # Trim top and bottom whitespace

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.show()

    # Print the top n-grams
    print("Top N-grams:\n", ngram_df)

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for age_type1 in ngram_df_by_age_type:
    for age_type2 in ngram_df_by_age_type:
        if age_type1 != age_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(ngram_df_by_age_type[age_type1]) & set(ngram_df_by_age_type[age_type2])

            # Store the number of common words
            common_word_counts[age_type1][age_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of N-Grams between Ages")
plt.show()

"""### N-gram Analysis - With Stopwords Included by Priority Score"""

ngram_df_by_priority_type = {}

for priority_type in requests['Priority Score'].unique():
    priority_type_requests = requests[requests['Priority Score'] == priority_type]

    # Prepare comments for the model
    comments = priority_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=30)
    X = vectorizer.fit_transform(comments)

    # Get n-gram frequency
    ngram_counts = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))

    # Sort n-grams by frequency
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    # Apply filtering
    filtered_ngrams = remove_redundant_ngrams(sorted_ngrams)

    # Convert to DataFrame for visualization
    ngram_df = pd.DataFrame(filtered_ngrams, columns=["N-gram", "Frequency"])

    ngram_df_by_priority_type[priority_type] = list(ngram_df['N-gram'])

    # # Plot the most common n-grams
    # plt.figure(figsize=(10, 5))
    # plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")
    # plt.xlabel("Frequency")
    # plt.ylabel("Common Phrases")
    # plt.title(f"Frequent Phrases (Including Stopwords) in Contact Forms for Priority Score {priority_type}")
    # plt.gca().invert_yaxis()
    # plt.show()

    # # Print the top n-grams
    # print("Top N-grams:\n", ngram_df)

    print('\n\n', priority_type, '\n\n')


     # Dynamic figure height based on number of bars
    bar_height = 0.5  # Reduce this to add more space between bars
    num_bars = len(ngram_df)
    fig_height = num_bars * bar_height *0.36  # 0.2 adds spacing
    plt.figure(figsize=(10, fig_height))
    #plt.figure(figsize=(10, 7))
    #plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")

    bars = plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue", height=bar_height)
    plt.bar_label(bars, fmt='%d', padding=3, color='darkblue', fontsize=8)

    plt.xlabel("Frequency")
    plt.ylabel("Common Phrases")
    #plt.title(f"Frequent Phrases (Including Stopwords) in Contact Forms for Priority Score {priority_type}")

    plt.gca().set_ylim(-0.5, num_bars - 0.5)  # Trim top and bottom whitespace

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.show()

    # Print the top n-grams
    print("Top N-grams:\n", ngram_df)

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for priority_type1 in ngram_df_by_priority_type:
    for priority_type2 in ngram_df_by_priority_type:
        if priority_type1 != priority_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(ngram_df_by_priority_type[priority_type1]) & set(ngram_df_by_priority_type[priority_type2])

            # Store the number of common words
            common_word_counts[priority_type1][priority_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of N-Grams between Priority Scores")
plt.show()

"""### N-gram Analysis - With Stopwords Included by Month"""

ngram_df_by_month_type = {}

month_year = requests
month_year['MONTH & YEAR'] = month_year['MONTH'] + ' ' + month_year['YEAR'].astype(str)

for month_type in month_year['MONTH & YEAR'].unique():
    month_type_requests = month_year[month_year['MONTH & YEAR'] == month_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    # Prepare comments for the model
    comments = month_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=30)
    X = vectorizer.fit_transform(comments)

    # Get n-gram frequency
    ngram_counts = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))

    # Sort n-grams by frequency
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    # Apply filtering
    filtered_ngrams = remove_redundant_ngrams(sorted_ngrams)

    # Convert to DataFrame for visualization
    ngram_df = pd.DataFrame(filtered_ngrams, columns=["N-gram", "Frequency"])

    ngram_df_by_month_type[month_type] = list(ngram_df['N-gram'])

    # # Plot the most common n-grams
    # plt.figure(figsize=(10, 5))
    # plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")
    # plt.xlabel("Frequency")
    # plt.ylabel("Common Phrases")
    # plt.title(f"Frequent Phrases (Including Stopwords) in Contact Forms for {month_type}")
    # plt.gca().invert_yaxis()
    # plt.show()

    print('\n\n', month_type, '\n\n')

    # # Print the top n-grams
    # print("Top N-grams:\n", ngram_df)

     # Dynamic figure height based on number of bars
    bar_height = 0.5  # Reduce this to add more space between bars
    num_bars = len(ngram_df)
    fig_height = num_bars * bar_height *0.36  # 0.2 adds spacing
    plt.figure(figsize=(10, fig_height))
    #plt.figure(figsize=(10, 7))
    #plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")

    bars = plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue", height=bar_height)
    plt.bar_label(bars, fmt='%d', padding=3, color='darkblue', fontsize=8)

    plt.xlabel("Frequency")
    plt.ylabel("Common Phrases")
    #plt.title(f"Frequent Phrases (Including Stopwords) in Contact Forms for {month_type}")

    plt.gca().set_ylim(-0.5, num_bars - 0.5)  # Trim top and bottom whitespace

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.show()

    # Print the top n-grams
    print("Top N-grams:\n", ngram_df)

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for assistance_type1 in ngram_df_by_month_type:
    for assistance_type2 in ngram_df_by_month_type:
        if assistance_type1 != assistance_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(ngram_df_by_month_type[assistance_type1]) & set(ngram_df_by_month_type[assistance_type2])

            # Store the number of common words
            common_word_counts[assistance_type1][assistance_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df.sort_values(by=common_word_counts_df.columns[0],ascending=False), annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of N-Grams between Months")
plt.show()

"""## N-gram Analysis - Stopwords Removed

### N-gram Analysis - Stopwords Removed by Assistance Type
"""

ngram_df_by_assistance_type = {}

for assitance_type in requests['ASSISTANCE TYPE'].unique():
    assitance_type_requests = requests[requests['ASSISTANCE TYPE'] == assitance_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    # Prepare comments for the model
    comments = assitance_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=30)
    X = vectorizer.fit_transform(preprocessed_comments)

    # Get n-gram frequency
    ngram_counts = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))

    # Sort n-grams by frequency
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    # Function to remove sub-ngrams
    def remove_redundant_ngrams(ngrams):
        filtered_ngrams = []
        for ngram, freq in ngrams:
            # Only add if it's not a subset of an already added n-gram
            if not any(ngram in longer_ngram for longer_ngram, _ in filtered_ngrams):
                filtered_ngrams.append((ngram, freq))
        return filtered_ngrams

    # Apply filtering
    filtered_ngrams = remove_redundant_ngrams(sorted_ngrams)

    # Convert to DataFrame for visualization
    ngram_df = pd.DataFrame(filtered_ngrams, columns=["N-gram", "Frequency"])

    ngram_df_by_assistance_type[assitance_type] = list(ngram_df['N-gram'])

    # # Plot the most common n-grams
    # plt.figure(figsize=(10, 5))
    # plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")
    # plt.xlabel("Frequency")
    # plt.ylabel("Common Phrases")
    # plt.title(f"Frequent Phrases (Stopwords Removed) in Contact Forms for {assitance_type}")
    # plt.gca().invert_yaxis()
    # plt.show()

    # # Print the top n-grams
    # print("Top N-grams:\n", ngram_df)

    print('\n\n', assitance_type, '\n\n')

     # Dynamic figure height based on number of bars
    bar_height = 0.5  # Reduce this to add more space between bars
    num_bars = len(ngram_df)
    fig_height = num_bars * bar_height *0.36  # 0.2 adds spacing
    plt.figure(figsize=(10, fig_height))
    #plt.figure(figsize=(10, 7))
    #plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")

    bars = plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue", height=bar_height)
    plt.bar_label(bars, fmt='%d', padding=3, color='darkblue', fontsize=8)

    plt.xlabel("Frequency")
    plt.ylabel("Common Phrases")
   # plt.title(f"Frequent Phrases (Not Including Stopwords) in Contact Forms for {assitance_type}")

    plt.gca().set_ylim(-0.5, num_bars - 0.5)  # Trim top and bottom whitespace

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.show()

    # Print the top n-grams
    print("Top N-grams:\n", ngram_df)

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for assistance_type1 in ngram_df_by_assistance_type:
    for assistance_type2 in ngram_df_by_assistance_type:
        if assistance_type1 != assistance_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(ngram_df_by_assistance_type[assistance_type1]) & set(ngram_df_by_assistance_type[assistance_type2])

            # Store the number of common words
            common_word_counts[assistance_type1][assistance_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of N-Grams between Assistance Types")
plt.show()

"""### N-gram Analysis - Stopwords Removed by Age"""

ngram_df_by_age_type = {}

for age_type in requests['AGE RANGE'].unique():
    age_type_requests = requests[requests['AGE RANGE'] == age_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    # Prepare comments for the model
    comments = age_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=30)
    X = vectorizer.fit_transform(preprocessed_comments)

    # Get n-gram frequency
    ngram_counts = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))

    # Sort n-grams by frequency
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    # Function to remove sub-ngrams
    def remove_redundant_ngrams(ngrams):
        filtered_ngrams = []
        for ngram, freq in ngrams:
            # Only add if it's not a subset of an already added n-gram
            if not any(ngram in longer_ngram for longer_ngram, _ in filtered_ngrams):
                filtered_ngrams.append((ngram, freq))
        return filtered_ngrams

    # Apply filtering
    filtered_ngrams = remove_redundant_ngrams(sorted_ngrams)

    # Convert to DataFrame for visualization
    ngram_df = pd.DataFrame(filtered_ngrams, columns=["N-gram", "Frequency"])

    ngram_df_by_age_type[age_type] = list(ngram_df['N-gram'])

    # # Plot the most common n-grams
    # plt.figure(figsize=(10, 5))
    # plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")
    # plt.xlabel("Frequency")
    # plt.ylabel("Common Phrases")
    # plt.title(f"Frequent Phrases (Stopwords Removed) in Contact Forms for {age_type}")
    # plt.gca().invert_yaxis()
    # plt.show()

    # # Print the top n-grams
    # print("Top N-grams:\n", ngram_df)

    print('\n\n', age_type, '\n\n')

     # Dynamic figure height based on number of bars
    bar_height = 0.5  # Reduce this to add more space between bars
    num_bars = len(ngram_df)
    fig_height = num_bars * bar_height *0.36  # 0.2 adds spacing
    plt.figure(figsize=(10, fig_height))
    #plt.figure(figsize=(10, 7))
    #plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")

    bars = plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue", height=bar_height)
    plt.bar_label(bars, fmt='%d', padding=3, color='darkblue', fontsize=8)

    plt.xlabel("Frequency")
    plt.ylabel("Common Phrases")
    #plt.title(f"Frequent Phrases (Not Including Stopwords) in Contact Forms for {age_type}")

    plt.gca().set_ylim(-0.5, num_bars - 0.5)  # Trim top and bottom whitespace

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.show()

    # Print the top n-grams
    print("Top N-grams:\n", ngram_df)

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for age_type1 in ngram_df_by_age_type:
    for age_type2 in ngram_df_by_age_type:
        if age_type1 != age_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(ngram_df_by_age_type[age_type1]) & set(ngram_df_by_age_type[age_type2])

            # Store the number of common words
            common_word_counts[age_type1][age_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of N-Grams between Age")
plt.show()

"""### N-gram Analysis - Stopwords Removed by Priority Score"""

ngram_df_by_priority_type = {}

for priority_type in requests['Priority Score'].unique():
    priority_type_requests = requests[requests['Priority Score'] == priority_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    # Prepare comments for the model
    comments = priority_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=30)
    X = vectorizer.fit_transform(preprocessed_comments)

    # Get n-gram frequency
    ngram_counts = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))

    # Sort n-grams by frequency
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    # Function to remove sub-ngrams
    def remove_redundant_ngrams(ngrams):
        filtered_ngrams = []
        for ngram, freq in ngrams:
            # Only add if it's not a subset of an already added n-gram
            if not any(ngram in longer_ngram for longer_ngram, _ in filtered_ngrams):
                filtered_ngrams.append((ngram, freq))
        return filtered_ngrams

    # Apply filtering
    filtered_ngrams = remove_redundant_ngrams(sorted_ngrams)

    # Convert to DataFrame for visualization
    ngram_df = pd.DataFrame(filtered_ngrams, columns=["N-gram", "Frequency"])

    ngram_df_by_priority_type[priority_type] = list(ngram_df['N-gram'])

    # # Plot the most common n-grams
    # plt.figure(figsize=(10, 5))
    # plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")
    # plt.xlabel("Frequency")
    # plt.ylabel("Common Phrases")
    # plt.title(f"Frequent Phrases (Stopwords Removed) in Contact Forms for Priority Score {priority_type}")
    # plt.gca().invert_yaxis()
    # plt.show()

    # # Print the top n-grams
    # print("Top N-grams:\n", ngram_df)

    print('\n\n', priority_type, '\n\n')

     # Dynamic figure height based on number of bars
    bar_height = 0.5  # Reduce this to add more space between bars
    num_bars = len(ngram_df)
    fig_height = num_bars * bar_height *0.36  # 0.2 adds spacing
    plt.figure(figsize=(10, fig_height))
    #plt.figure(figsize=(10, 7))
    #plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")

    bars = plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue", height=bar_height)
    plt.bar_label(bars, fmt='%d', padding=3, color='darkblue', fontsize=8)

    plt.xlabel("Frequency")
    plt.ylabel("Common Phrases")
    #plt.title(f"Frequent Phrases (Not Including Stopwords) in Contact Forms for Priority Score {priority_type}")

    plt.gca().set_ylim(-0.5, num_bars - 0.5)  # Trim top and bottom whitespace

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.show()

    # Print the top n-grams
    print("Top N-grams:\n", ngram_df)

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for priority_type1 in ngram_df_by_priority_type:
    for priority_type2 in ngram_df_by_priority_type:
        if priority_type1 != priority_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(ngram_df_by_priority_type[priority_type1]) & set(ngram_df_by_priority_type[priority_type2])

            # Store the number of common words
            common_word_counts[priority_type1][priority_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df, annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of N-Grams between Priority Score")
plt.show()

"""### N-gram Analysis - Stopwords Removed by Month"""

ngram_df_by_month_type = {}

month_year = requests
month_year['MONTH & YEAR'] = month_year['MONTH'] + ' ' + month_year['YEAR'].astype(str)

for month_type in month_year['MONTH & YEAR'].unique():
    month_type_requests = month_year[month_year['MONTH & YEAR'] == month_type]
    # print(assitance_type, '\n', assitance_type_requests['COMMENTS'].head())

    # Prepare comments for the model
    comments = month_type_requests['COMMENTS']

    # Get rid of empty comments
    comments = comments.dropna()

    preprocessed_comments = comments.apply(preprocess_text)

    vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=30)
    X = vectorizer.fit_transform(preprocessed_comments)

    # Get n-gram frequency
    ngram_counts = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))

    # Sort n-grams by frequency
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    # Function to remove sub-ngrams
    def remove_redundant_ngrams(ngrams):
        filtered_ngrams = []
        for ngram, freq in ngrams:
            # Only add if it's not a subset of an already added n-gram
            if not any(ngram in longer_ngram for longer_ngram, _ in filtered_ngrams):
                filtered_ngrams.append((ngram, freq))
        return filtered_ngrams

    # Apply filtering
    filtered_ngrams = remove_redundant_ngrams(sorted_ngrams)

    # Convert to DataFrame for visualization
    ngram_df = pd.DataFrame(filtered_ngrams, columns=["N-gram", "Frequency"])

    ngram_df_by_month_type[month_type] = list(ngram_df['N-gram'])

    # # Plot the most common n-grams
    # plt.figure(figsize=(10, 5))
    # plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")
    # plt.xlabel("Frequency")
    # plt.ylabel("Common Phrases")
    # plt.title(f"Frequent Phrases (Stopwords Removed) in Contact Forms for {month_type}")
    # plt.gca().invert_yaxis()
    # plt.show()

    # # Print the top n-grams
    # print("Top N-grams:\n", ngram_df)

    print('\n\n', month_type, '\n\n')

     # Dynamic figure height based on number of bars
    bar_height = 0.5  # Reduce this to add more space between bars
    num_bars = len(ngram_df)
    fig_height = num_bars * bar_height *0.36  # 0.2 adds spacing
    plt.figure(figsize=(10, fig_height))
    #plt.figure(figsize=(10, 7))
    #plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue")

    bars = plt.barh(ngram_df["N-gram"], ngram_df["Frequency"], color="skyblue", height=bar_height)
    plt.bar_label(bars, fmt='%d', padding=3, color='darkblue', fontsize=8)

    plt.xlabel("Frequency")
    plt.ylabel("Common Phrases")
    #plt.title(f"Frequent Phrases (Not Including Stopwords) in Contact Forms for {month_type}")

    plt.gca().set_ylim(-0.5, num_bars - 0.5)  # Trim top and bottom whitespace

    plt.gca().invert_yaxis()
    plt.tight_layout()

    plt.show()

    # Print the top n-grams
    print("Top N-grams:\n", ngram_df)

common_word_counts = defaultdict(lambda: defaultdict(int))

# Iterate through all pairs of assistance types
for assistance_type1 in ngram_df_by_month_type:
    for assistance_type2 in ngram_df_by_month_type:
        if assistance_type1 != assistance_type2:  # Don't compare with itself
            # Get common words between the two assistance types
            common_words = set(ngram_df_by_month_type[assistance_type1]) & set(ngram_df_by_month_type[assistance_type2])

            # Store the number of common words
            common_word_counts[assistance_type1][assistance_type2] = len(common_words)

# Convert to a pandas DataFrame for better visualization
common_word_counts_df = pd.DataFrame(common_word_counts).fillna(0).astype(int)  # Fill with 0 where no common words and convert to int

plt.figure(figsize=(10, 8))
sns.heatmap(common_word_counts_df.sort_values(by=common_word_counts_df.columns[0],ascending=False), annot=True, cmap="YlGnBu", fmt="d")  # fmt="d" for integer display
plt.title("Number of N-Grams between Months")
plt.show()
